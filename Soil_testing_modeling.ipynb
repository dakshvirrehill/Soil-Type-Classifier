{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "data_dir = \"D:/Durhum/2 sem/Soil_Dataset\"\n",
    "IMG_SIZE = 32\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through all the folders and save the images & labels in list variable.\n",
    "#return data is a list variable, it contain (image,label)\n",
    "def get_data(folder):\n",
    "    data = []\n",
    "    folder_path = os.path.join(data_dir,folder)\n",
    "    for img_fol in os.listdir(folder_path):\n",
    "        label = str(img_fol)\n",
    "        img_fol_path = os.path.join(folder_path,img_fol)\n",
    "        for imgs in os.listdir(img_fol_path):\n",
    "            imgs_folder = os.path.join(img_fol_path, imgs)\n",
    "            img = cv2.imread(imgs_folder,cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "            data.append([img,label])\n",
    "    shuffle(data)\n",
    "    return data\n",
    "\n",
    "#Check for train dataset and test dataset and seperate them for model training and testing\n",
    "def train_test_dataset():\n",
    "    for folder in os.listdir(data_dir):\n",
    "        if folder == \"Train\":\n",
    "            train_data = get_data(folder)\n",
    "        else:\n",
    "            test_data = get_data(folder)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train_x, train_y and test_x, test_y list\n",
    "train_X = []\n",
    "train_y = []\n",
    "test_X = []\n",
    "test_y = []\n",
    "for i in train_data:\n",
    "    train_X.append(i[0])\n",
    "    train_y.append(i[1])\n",
    "\n",
    "for i in test_data:\n",
    "    test_X.append(i[0])\n",
    "    test_y.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing for image data.\n",
    "#converting into float32 variable\n",
    "train_X=np.array(train_X)\n",
    "test_X=np.array(test_X)\n",
    " \n",
    "train_X=train_X/255.0\n",
    "test_X=test_X/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding for label data\n",
    "def onehot(data):\n",
    "    label_encoder=LabelEncoder()\n",
    "    label_ids=label_encoder.fit_transform(data)\n",
    "    onehot_encoder=OneHotEncoder(sparse=False)\n",
    "    reshaped=label_ids.reshape(len(label_ids), 1)\n",
    "    return onehot_encoder.fit_transform(reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(onehot(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(onehot(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\NiksBot\\envs\\aidi1100\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\NiksBot\\envs\\aidi1100\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sequential model\n",
    "model=Sequential()\n",
    "model.add(Conv2D(715,(3,3),input_shape=(32,32,3),\n",
    "    padding='same',activation='relu',\n",
    "    kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32,(3,3),activation='relu',padding='same',kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd=SGD(lr=0.01,momentum=0.9,decay=(0.01/25),nesterov=False)\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer=sgd,\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 715)       20020     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 715)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        205952    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 4,422,840\n",
      "Trainable params: 4,422,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\NiksBot\\envs\\aidi1100\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 715 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      "715/715 [==============================] - 19s 26ms/step - loss: 1.2205 - accuracy: 0.4378 - val_loss: 0.9588 - val_accuracy: 0.5691\n",
      "Epoch 2/10\n",
      "715/715 [==============================] - 22s 30ms/step - loss: 0.5187 - accuracy: 0.7748 - val_loss: 0.5486 - val_accuracy: 0.7340\n",
      "Epoch 3/10\n",
      "715/715 [==============================] - 23s 32ms/step - loss: 0.4704 - accuracy: 0.8028 - val_loss: 0.3720 - val_accuracy: 0.8298\n",
      "Epoch 4/10\n",
      "715/715 [==============================] - 23s 32ms/step - loss: 0.5287 - accuracy: 0.7818 - val_loss: 0.5601 - val_accuracy: 0.7713\n",
      "Epoch 5/10\n",
      "715/715 [==============================] - 23s 32ms/step - loss: 0.4006 - accuracy: 0.8531 - val_loss: 0.5242 - val_accuracy: 0.7926\n",
      "Epoch 6/10\n",
      "715/715 [==============================] - 23s 32ms/step - loss: 0.4151 - accuracy: 0.8182 - val_loss: 0.4528 - val_accuracy: 0.8298\n",
      "Epoch 7/10\n",
      "715/715 [==============================] - 22s 31ms/step - loss: 0.3133 - accuracy: 0.8797 - val_loss: 0.4576 - val_accuracy: 0.7926\n",
      "Epoch 8/10\n",
      "715/715 [==============================] - 22s 31ms/step - loss: 0.3154 - accuracy: 0.8741 - val_loss: 0.4613 - val_accuracy: 0.8245\n",
      "Epoch 9/10\n",
      "715/715 [==============================] - 23s 32ms/step - loss: 0.2710 - accuracy: 0.8881 - val_loss: 0.5185 - val_accuracy: 0.8245\n",
      "Epoch 10/10\n",
      "715/715 [==============================] - 22s 31ms/step - loss: 0.2911 - accuracy: 0.8615 - val_loss: 0.3157 - val_accuracy: 0.8617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13ffceb0f88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, validation_data=(test_X,test_y), epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 5ms/step\n",
      "86.17021441459656\n"
     ]
    }
   ],
   "source": [
    "_,acc=model.evaluate(test_X,test_y)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_soil_testing_10epoch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
